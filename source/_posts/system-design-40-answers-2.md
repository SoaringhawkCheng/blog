---
title: 「高并发系统设计40问」学习笔记 （下）
catalog: true
date: 2022-04-15 17:18:58
subtitle:
header-img:
tags:
- architecture
categories:
- 工程
---

> 参考资料链接：[高并发系统设计40问](https://zq99299.github.io/note-architect/hc/) [系统设计题精选](https://soulmachine.gitbooks.io/system-design/content/cn/)
> 
> 书籍豆瓣链接： 
> 
> 开始学习时间：
> 
> 预计完成时间：
> 
> 实际完成时间：

# 四、消息队列

## 4.1 如何处理大量写请求

异步处理、解耦合和削峰填谷 是消息队列在秒杀系统设计中起到的主要作用


## 4.2处理消息丢失

### 4.2.1 消息生产阶段

使用重传，重试次数2-3次，但仍然有丢失的风险

### 4.2.2 消息队列阶段

为减少磁盘的随机I/O，会将消息写入操作系统的Page Cache中，然后异步刷入磁盘，这样会造成未持久化宕机造成的消息丢失

redis集群有leader提供消息写入和消费，多个follower负责数据的备份，leader故障就从follower的ISR选主

如果需要确保消息不丢失，生产者需要设置`acks=all`，每一条消息必须得到所有Leader和ISR确认才会被认为发送成功

### 4.2.3 消费过程阶段

一定要等到消息接收和处理完成后才能更新消费进度，但是这也会造成消息重复的问题

## 4.3 如何保证消息只被消息一次

想要完全的避免消息重复的发生是很难做到的，只能保证在消息的生产和消费的过程是幂等的

### 4.3.1 消息不重复生产

给每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一 ID，消息队列的服务端会存储`<生产者 ID，最后一条消息ID>`的映射，丢失重复消息

### 4.3.2 消费端

通过版本号，发送消息查询版本号，消费后增加版本号，丢弃低版本号消息

## 4.4 如何降低消息队列系统消息的延迟

### 4.4.1 监控消息延迟

Kakfa 的一个专门的 topic 叫 `__consumer_offsets`，Kafka 安装包的 bin 目录下有个工具`kafka-consumer-groups.sh`，可以查看消息消费情况

### 4.4.2 优化消息延迟

#### 4.4.2.1 消费端

1. 提升consumer代码性能

2. 一个partition对应一个consumer，否则需要加锁。consumer创建一个线程池，一次拉取多条消息，分配给多个线程处理

3. 客户端拉取消息，采用递增步长，10ms拉不到，下次20ms，20ms拉不到下次100ms

#### 4.4.2.2 消息队列

1. 数据库写入qps只能到千级别，消息存储使用本地磁盘而不是数据库，page cache可以提升读取速度，因为消息读取是顺序的

2. 零拷贝，sendfile直接将数据从内核缓冲区拷贝到socket缓冲区，不经过用户缓冲区

# 五、分布式服务

## 5.1 每秒1w次请求的系统需要服务化拆分嘛

### 5.1.1 一体化架构的痛点

数据库连接数可能成为系统瓶颈

增加了研发的成本

增加系统运维的压力

### 5.1.2 如何使用微服务化解决这些痛点

数据库垂直分库，业务层拆分

增加中间层，将不同数据库的直接访问，转变为服务化抽象出公共服务

## 5.2 微服务化后，系统架构如何改造

### 5.2.1 微服务拆分原则

高内聚，低耦

先粗略拆分，随着业务的发展，再逐渐细化

拆分的过程中，尽量避免影响产品的日常功能迭代

服务接口的定义要具备可扩展性

### 5.2.2 微服务化带来的问题和解决思路

#### 5.2.2.1 服务间的调用变成进程间的网络调用

引入服务注册中心，管理的是服务完整的生命周期，包括对于服务存活状态的检测

#### 5.2.2.2 服务间依赖关系错综复杂

引入服务治理体系，采用熔断、降级、限流、超时控制的方法，使得问题被限制在单一服务中

#### 5.2.2.3 调用链路问题定位

引入分布式追踪工具，以及更细致的服务端监控报表

## 5.3 实现支持10w QPS的RPC框架

提升 RPC 框架的性能，需要从 网络传输和序列化 两方面来优化

### 5.3.1 网络传输优化

选择高性能的 I/O 模型，这里我推荐使用同步多路 I/O 复用模型

调试网络参数，比如将 tcp_nodelay 设置为 true，比如接受缓冲区和发送缓冲区的大小，客户端连接请求缓冲队列的大小（back log）

### 5.3.2 合适的序列化方式

性能要求不高，在传输数据占用带宽不大的场景下，可以使用 JSON

性能要求比较高，那么使用 Thrift 或者 Protobuf 都可以

一些存储的场景下，比如说你的缓存中存储的数据占用空间较大，那么你可以考虑使用 Protobuf 替换 JSON，作为存储数据的序列化方式

## 5.4 注册中心：分布式系统如何寻址

### 5.4.1 Nginx反向代理与注册中心

Nginx通过在配置文件中配置应用服务器的ip

注册中心：提供了服务地址的存储；存储内容发生变化时，可以将变更的内容推送给客户端

### 5.4.2 如何做服务管理

心跳机制

注册中心为每一个连接上来的 RPC 服务节点，记录最近续约的时间 ，RPC 服务节点在启动注册到注册中心后，就按照一定的时间间隔（比如 30 秒），向注册中心发送心跳包

## 5.5 分布式Trace

## 5.6 负载均衡

### 5.6.1 负载均衡种类

#### 5.6.1.1 代理类负载均衡服务

* LVS

LVS 在 OSI 网络模型中的第四层，传输层工作，所以 LVS 又可以称为四层负载，

4层负载均衡基本就是基于 IP + 端口进行负载均衡，LVS在四层做请求包的转发，请求包转发之后，由客户端和后端服务直接建立连接，后续的响应包不会再经过 LVS 服务器

* Nginx

Nginx 运行在 OSI 网络模型中的第七层，应用层，所以又可以称它为七层负载

7层负载均衡可以基于不同的协议，除了根据IP加端口进行负载外，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡
 
 * 选型
 
 QPS十万以内，使用了Nginx作为唯一的负载均衡服务器，减少系统复杂度和维护成本
 
流量更大时，LVS 适合在入口处，承担大流量的请求分发； Nginx 要部署在业务服务器之前做更细维度的请求分发和故障节点检测

#### 5.6.1.2 客户端负载均衡服务

LVS和Nginx适用于普通对的Web服务，不适用于微服务架构:

1. 微服务架构服务节点存在注册中心，使用LVS很难和注册中心交互
2. 微服务架构使用RPC协议而不是HTTP协议，Nginx不能满足要求

微服务架构可以将把负载均衡的服务内嵌在 RPC 客户端中，使用客户端负载均衡服务

注册中心提供服务节点的完整列表，客户端拿到列表之后使用负载均衡服务的策略选取一个合适的节点，然后将请求发到这个节点上

### 5.6.2 负载均衡策略

负载均衡策略从大体上来看可以分为两类：

#### 5.6.2.1 静态策略

一类是静态策略，也就是说负载均衡服务器在选择服务节点时，不会参考后端服务的实际运行的状态

如轮询，IP和URL哈希

#### 5.6.2.2 动态策略

一类是动态策略，也就是说负载均衡服务器会依据后端服务的一些负载特性，来决定要选择哪一个服务节点

### 5.6.3 使用Nginx进行负载均衡

#### 5.6.3.1 负载均衡配置

略

#### 5.6.3.2 感知节点故障

nginx模块`nginx_upstream_check_module`，定期地探测后端服务的一个指定的接口，然后根据返回的状态码，来判断服务是否还存活。当探测不存活的次数达到一定阈值时，就自动将这个后端服务从负载均衡服务器中摘除

#### 5.6.3.3 感知节点新增节点

有一种 consul + nginx 方案，就是把节点信息写在 consul 里面，这样当节点变化时，nginx 可以得到通知

## 5.7 API网关

### 5.7.1 API网关的作用

API 网关（API Gateway）是一种架构模式，将一些服务共有的功能整合在一起，独立部署为单独的一层，用来解决一些服务治理的问题。你可以把它看作系统的边界，它可以对出入系统的流量做统一的管控

API 网关可以分为两类：一类叫做入口网关，一类叫做出口网关。

#### 5.7.1.1 入口网关

部署在负载均衡服务器和应用服务器之间：

1. 它提供客户端一个统一的接入地址，API 网关可以将用户的请求动态路由到不同的业务服务上，并且做一些必要的协议转换工作
2. 植入一些服务治理的策略
3. 客户端的认证和授权的实现
4. 做一些与黑白名单相关的事情
5. 做一些日志记录的事情

#### 5.7.1.2 出口网关

出口网关，对调用外部的API做统一的认证、授权、审计以及访问控制

### 5.7.2 TODO

## 5.8 多机房部署

### 5.8.1 部署方案

两个机房 A 和 B 都部署了应用服务，数据库的主库和从库部署在 A 机房，那么机房 B 的应用如何访问到数据呢？ 有两种思路

1. 一个思路是直接跨机房读取 A 机房的从库
2. 机房B部署一个从库，跨机房同步主库数据，然后机房B读取同机房从库数据

### 5.8.2 数据传输时延迟

假设接口TP99要求是200ms

1. 北京同城双机房专线，延迟1-3ms，接口读取缓存和数据库的数量少于10次可以接受
2. 国内异地双机房专线，延迟50ms以内，尽量减少跨机房服务调用、数据库和缓存操作
3. 国内访问美国西海岸服务，延时100-200ms，避免跨甲方同步调用，只能做异步的数据同步

### 5.8.3 业务方案选型

#### 5.8.3.1 同城双活

主库部署在A，A和B都部署从库，一旦A机房发生故障，通过主从切换将B机房的从库提升为主库

不同机房的服务向注册中心注册不同的服务组，尽量避免跨机房的RPC调用，但是还是会存在跨机房写数据库的问题

缓存部署在两个机房中，查询请求只读取本机房的缓存

#### 5.8.3.2 异地多活

随着业务不断发展，就要考虑所在城市发生重大灾害，也要保证系统可用性，需要采用异地多活方案

异地不能离的太近，否则自然灾害可能同时波及，这就造成更高的数据传输延迟，同城双活的跨机房写数据库方案便不再合适

数据同步方案两种方案相结合：

1. 基于存储系统的主从复制，如MySQL和Redis
2. 基于消息队列的方式，B机房产生的写入请求写入消息队列，A机房消费后执行业务逻辑和数据写入


## 5.9 ServiceMesh

将服务治理的细节，从 RPC 客户端中拆分出来，形成一个代理层单独部署。这个代理层可以使用单一的语言实现，所有的流量都经过代理层，来使用其中的服务治理策略。这是一种 关注点分离 的实现方式，也是 Service Mesh 的核心思想

### 5.9.1 sidecar

在应用程序同主机上部署一个代理程序Sidecar，RPC 客户端将数据包先发送给，与自身同主机部署的 Sidecar，在 Sidecar 中经过服务发现、负载均衡、服务路由、流量控制之后，再将数据发往指定服务节点的 Sidecar，在服务节点的 Sidecar 中，经过记录访问日志、记录分布式追踪日志、限流之后，再将数据发送给 RPC 服务端

### 5.9.2 Istio

业界提及最多的 Service Mesh 方案当属 istio，将组件分为 数据平面 和 控制平面：

1. 数据平面即sidecar
2. 控制平面主要负责服务治理策略的执行


# 六、维护

## 6.1 服务端监控

### 6.1.1 监控指标

延迟，吞吐量、错误和使用率

### 6.1.2 如何采集

#### 6.1.2.1 Agent

Agent 主要收集的是组件服务端的信息，比如用agent链接memcached服务器，并且发送一个stats命令，获取服务器的统计信息

#### 6.1.2.2 代码埋点

每个10s对资源的数据进行汇总，发送给监控服务器

#### 6.1.2.3 日志

通过日志采集工具，将Tomcat、Nginx等日志中的数据发送给监控服务器

### 6.1.3 监控数据处理和存储

一般会先用消息队列来承接数据，主要的作用是削峰填谷，防止写入过多的监控数据，让监控服务产生影响。一般会部署两个队列处理程序，来消费消息队列中的数据

#### 6.1.3.1 ES

将数据写入ES，通过Kibana展示原始数据，支持全文搜索

#### 6.1.3.2 流式处理中间件

使用spark、strom等，从消息队列里接收数据并做一些处理：

* 解析数据格式

* 聚合运算

比如针对Tomcat访问日志，可以计算同一个URL一断时间内的请求量、响应时间和错误

* 数据存储到时间序列数据库

常用的时序数据库有 InfluxDB、OpenTSDB、Graphite

* Grafana链接时序数据库，绘制报表

## 6.2 性能管理

应用性能管理（Application Performance Management，简称 APM）。服务端监控的核心关注点是后端服务的性能和可用性，而应用性能管理的核心关注点是终端用户的使用体验，也就是你需要衡量，从客户端请求发出开始，到响应数据返回到客户端为止，这个端到端的整体链路上的性能情况。

### 6.2.1 如何搭建APM系统

* 数据采集：客户单sdk采集信息并并定期上报
*  数据存储：类似工作中遇到的客户端埋点格式
*  数据展示：与服务端监控一样

### 6.2.2 如何采取网络数据

安卓一般会使用 OkHttpClient 来请求接口数据，而 OkHttpClient 提供了 EventListner 接口，可以让调用者接收到网络请求事件，可以得出一次请求过程中，经历的一系列过程的时间，其中主要包括下面几项：

* 等待时间：请求会首先缓存在本地的队列里面，由专门的 I/O 线程负责，那么在 I/O 线程真正处理请求之前，会有一个等待的时间
* DNS 时间
* 握手时间
* SSL 时间
* 发送时间
* 首包时间
* 包接收时间

## 6.3 压力测试

### 如何搭建全链路压测平台

全链路压测平台需要包含以下几个模块



### 6.3.1 压测数据产生

入口流量是来自于客户端的 HTTP 请求，拷贝一份存储在像是 HBase、MongoDB 这些 NoSQL 存储组件，或者亚马逊 S3 这些云存储服务中，我们称之为 流量数据工厂

拷贝方式：

* 直接拷贝负载均衡服务器的访问日志，数据就以文本的方式写入到流量数据工厂中
* 流量拷贝工具 GoReplay，它可以劫持本机某一个端口的流量，将它们记录在文件中

需要对 压测流量染色，也就是增加压测标记。在实际项目中，我会在 HTTP 的请求头中增加一个标记项，比如说叫做 is stress test

### 数据如何隔离

一些推荐服务，展示过的商品就不再会被推荐出来，需要Mock 这些推荐服务

### 压力测试如何实施

而是按照一定的步长，逐渐地增加流量。在增加一次流量之后，让系统稳定运行一段时间，观察系统在性能上的表现。如果发现依赖的服务或者组件出现了瓶颈，回退到上一次压测的 QPS，保证服务的稳定，再针对此服务或者组件进行扩容，然后再继续增加流量压测。

## 6.4 配置管理

## 6.5 降级熔断

## 6.6 流量控制

# 七、实战篇
